<!DOCTYPE HTML>
<html>

<head>
    <title>Vetri's Resume</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="assets/css/main.css" />
</head>

<body class="is-preload">

    <!-- Header -->
    <header id="header">
        <div class="inner">
            <a href="#" class="image avatar"><img src="images/profile.jpg" alt="" /></a>
            <h1><strong>VETRI J</strong><br />
                Data Engineer<br />
                +91 93443 65064<br />
                vetrijdataengineer@gmail.com<br />
        </div>
    </header>

    <!-- Main -->
    <div id="main">

        <!-- One -->
        <section id="one">
            <header class="major">
                <h2>Data Engineer</h2>
            </header>
            <p>Experienced Data Engineer proficient in AWS infrastructure (Glue, EMR, Redshift), Spark, PySpark, Azure
                Databricks, Data Lake, Airflow, and PL/SQL. Skilled in Python scripting, query optimization, automation,
                and pipeline development. Recognized for optimizing Redshift report generation and receiving awards for
                data governance and debut initiatives
            </p>
        </section>

        <!-- Two -->
        <section id="two">
            <h2>Experience</h2>
            <a href="https://www.applieddatafinance.com/">Data Engineer</a>
            <p>
                <strong>ADF Data Science Pvt. Ltd.</strong><br />
                <em>JUNE 2021</em><br />
            <h4>Centralized Reporting System with LLM Integration</h4>
            <ul>
                <li>Designed and implemented a reporting platform to consolidate data across business teams, utilizing
                    automation and AI for enhanced efficiency.</li>
                <li>Built an ETL pipeline to extract email reports, storing HTML content and attachments in S3 with a
                    structured organization.</li>
                <li>Integrated an LLM agent to summarize reports and provide historical insights within a single web
                    application.</li>
                <li>Optimized data processing workflows to generate faster reports and improve accessibility for
                    end-users.</li>
            </ul>

            <h4>Create Data Pipelines with different activities & data flow with different sources and destinations to
                satisfy giving business needs:</h4>
            <ul>
                <li>Develop data pipelines with diverse activities and flow patterns using AWS DMS</li>
                <li>Creating Data Quility Alert system for kinda posibilities</li>
                <li>Meet MLE business requirements with these pipelines</li>
            </ul>
            <h4>Extract data from various third-party sources using cutting-edge-tech stack:</h4>
            <ul>
                <li>Extract data using cutting-edge technology stacks like REST API, Web Scrapping, API, Graph API </li>
                <li>Utilizing MongoDB to extract tags for converting Machine Learning model tables</li>
            </ul>
            <h4>Scheduling and monitoring tasks via (CI/CD) Jenkins:</h4>
            <ul>
                <li>Utilize Jenkins for scheduling and monitoring tasks</li>
                <li>Take advantage of Jenkins' Continuous Integration and Continuous Deployment (CI/CD) capabilities
                </li>
            </ul>
            <h4>Implementing Data Catalog Tool (open source):</h4>
            <ul>
                <li>Implement an open-source Data Catalog Tool Apache Atlas</li>
                <li>Facilitate data discovery and management with this tool</li>
            </ul>
            <h4>ETL - Pentaho, Python, Pyspark</h4>
            <ul>
                <li>Utilize Pentaho for Extract, Transform, Load (ETL) operations.</li>
            </ul>
            </p>
        </section>

        <!-- Three -->
        <section id="three">
            <h2>Education</h2>
            <div class="table-wrapper">
                <table>
                    <thead>
                        <tr>
                            <th>Degree/Grade</th>
                            <th>Institution</th>
                            <th>Duration</th>
                            <th>Performance</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>MCA</td>
                            <td>SRM Institute of Science and Technologie, Chennai</td>
                            <td>JUNE 2019 - MAY 2021</td>
                            <td>82%</td>
                        </tr>
                        <tr>
                            <td>BCA</td>
                            <td>Aadhiparasakthi College of arts and science, vellore</td>
                            <td>JUNE 2016 - MAY 2019</td>
                            <td>76%</td>
                        </tr>
                        <tr>
                            <td>12th Grade</td>
                            <td>Anderson Hr.Sec. School, Kanchipuram</td>
                            <td>Year of Passing - 2016</td>
                            <td>69%</td>
                        </tr>
                        <tr>
                            <td>10th Grade</td>
                            <td>Anderson Hr.Sec. School, Kanchipuram</td>
                            <td>Year of Passing - 2014</td>
                            <td>85%</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </section>

        <!-- Four -->
        <section id="four">
            <h2>Skills</h2>

            <ul>
                <li><span class="icon brands fa-js"></span><strong> Python, Pyspark, Spark </strong></li>
                <li><span class="fa fa-tools"></span><strong> Data Engineering: </strong> AWS, Preprocessing, Data
                    Modelling, Data Governance, Data Stewards, Database Administrators, DevOps Engineer(Basis), API
                    integration, Automation, Optimization Techniques</li>
                <li><span class="fa fa-tools"></span><strong> Data Quailty: </strong> Data Profiling, Data Validation,
                    Data Enrichment, Data Monitoring</li>
                <li><span class="icon solid fa-database"></span><strong> DataBase Archcitechure: </strong> - SQL,
                    PL/SQL,
                    MongoDb, Big Query</li>
                <li><span class="icon solid fa-database"></span><strong> AWS </strong> - Redshift, DMS, RDS, Lambda,
                    Cloud
                    Watch, VPC, EMR </li>
                <li><span class="icon solid fa-database"></span><strong> Azure </strong> - Azure Databricks,Azure Data
                    Factory, ADLS Gen 2 </li>
                <li><span class="fa fa-book"></span><strong> CI/CD & Orchestrature:
                    </strong> - Jira, BitBucket, Terraform, Jenkins and Airflow</li>
                <li><span class="icon brands fa-html5"></span><strong>HTML , <span class="icon brands fa-sass"></span>
                        <strong>
                            Css</strong>-
                        Bootstrap</li>
                <li><span class="icon brands fa-python"></span><strong> Machine Learning Basics: </strong> -
                    Linear/Logistic
                    Regression, SVM,
                    Random Forest, Decision Tree, K nearest Neighbor, Survival Analysis</li>

            </ul>
        </section>

        </section>
        <p>

            <!-- Five -->
        <section id="five">
            <h2>Projects</h2>
            <a href="#">Azure Data Factory Promise Table Migration with PySpark Processing</a> I led the migration of
            promise tables into Azure Data Lake Storage Gen2 (ADLS Gen2) through Azure Data Factory, ensuring seamless
            data transfer and storage. Leveraging PySpark on Azure Databricks, I orchestrated efficient data processing
            pipelines for in-depth analysis. By optimizing ETL orchestration and implementing data quality checks,
            I ensured the reliability and accuracy of promise table data. The project focused on scalability and
            performance,
            enabling the processing of large datasets with ease. Through monitoring and logging mechanisms,
            I ensured the continuous improvement of data pipelines, facilitating informed decision-making.<br />

            <a href="#">Seamless Third-Party Data Integration </a> - Orchestrated the loading of third-party data into
            Redshift through API integration, web scraping, and Graph API for Outlook, ensuring a continuous influx of
            relevant data for analysis.<br />

            <a href="#">Creating Data Mart Tables Using CDC</a> CDC files from DMS S3 were extracted into Redshift with
            a proper naming convention Pyspark was used for transformation and mapping variables Daily jobs were run to
            fetch the most recent historical CDC files and write them to the target table in Redshift 5 reports were
            generated using the EDW tables as the data source<br />

            <a href="#">Streamlined Machine Learning QA Processes</a> Implemented a novel methodology for QA of MLE
            model tables,
            slashing manual QA time by 70% and accelerating the process by 65% compared to previous methods.<br />

            <a href="#">Implementation of Data Governance tool using Apache Atlas </a> - Apache Atlas was implemented
            using Docker RDS metadata was imported into Atlas via REST API Automated script created to ingest RDS
            entities into Atlas To ensure secure data governance, role-based policies were implemented and assigned to
            relevant teams The data dictionary is uploaded via Jenkins for each release Atlas automatically populates
            the data dictionary with necessary data.<br />

            <a href="https://vettrivikas.github.io/Nush_clothing/index.html">Nush Shopping</a> - Fashion E-commerce
            Shopping
            website
            using Bootstrap, HTML5, CSS <br />
            <!-- Six -->
            </p>
        </section>

        <section id="six">
            <h2>Certifications</h2>
            <p>
                <span class="icon solid fa-certificate"></span><a
                    href="https://udemy-certificate.s3.amazonaws.com/pdf/UC-772ec45f-ca9b-4b27-ab52-70b82b665df8.pdf">
                    Google BigQuery & PostgreSQL : Big Query for Data Analysis</a><br />
                <span class="icon solid fa-certificate"></span><a
                    href="https://udemy-certificate.s3.amazonaws.com/pdf/UC-9dc640a9-46fd-48ea-8d2a-1a4b41ed4698.pdf">
                    Data Engineering essential SQL, Python and Spark</a><br />
                <span class="icon solid fa-certificate"></span>MongoDB Basic M001<br />
                <span class="icon solid fa-certificate"></span> Core Java Certifications<br />
            </p>
        </section>

        <section id="six">
            <h2>Achievement</h2>
            <p>
                <span class="icon solid fa-certificate"></span>Received Promising Debut award for the year 2022<br />
            </p>
        </section>

    </div>

    <!-- Footer -->
    <footer id="footer">
        <div class="inner">
            <ul class="icons">
                <li><a href="https://www.linkedin.com/in/vetri-j-data-engineer/" class="icon brands fa-linkedin"><span
                            class="label">Twitter</span></a></li>
                <li><a href="https://github.com/vettrivikas" class="icon brands fa-github"><span
                            class="label">Github</span></a></li>
                <li><a href="https://www.facebook.com/vettri.vel.1293/" class="icon brands fa-facebook"><span
                            class="label">Dribbble</span></a></li>
                <li><a href="https://www.instagram.com/__vetri_._/" class="icon brands fa-instagram"><span
                            class="label">Dribbble</span></a></li>
                <!-- <li><a href="" class="icon solid fa-envelope"><span class="label">Email</span></a></li> -->
            </ul>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/jquery.poptrox.min.js"></script>
    <script src="assets/js/browser.min.js"></script>
    <script src="assets/js/breakpoints.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/main.js"></script>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-LMNXYNZQLX"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-LMNXYNZQLX');
    </script>

</body>

</html>